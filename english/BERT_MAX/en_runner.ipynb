{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hparams import EMOTIONX_MODEL_HPARAMS\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from models import EmotionX_Model\n",
    "from utils import load_data, shuffle_trainset, get_batch, make_dir, print_params\n",
    "\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7f1c549c70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_start():\n",
    "    if not torch.cuda.is_available():\n",
    "        raise NotImplementedError()\n",
    "    hparams = type('', (object,), EMOTIONX_MODEL_HPARAMS)() # dict to class\n",
    "\n",
    "    # data\n",
    "    fr_train_dialogs, fr_train_labels = load_data(hparams, hparams.fr_train_path)\n",
    "    test_dialogs, test_labels = load_data(hparams, hparams.fr_test_path)\n",
    "    train_dialogs = fr_train_dialogs\n",
    "    train_labels = fr_train_labels\n",
    "    assert len(train_dialogs) == len(train_labels)\n",
    "    assert len(test_dialogs) == len(test_labels)\n",
    "\n",
    "    # hyper-parameter\n",
    "    hparams.n_appear = [sum(train_labels, []).count(i) for i in range(8)]\n",
    "    max_i = len(train_dialogs)\n",
    "    total_step = 0\n",
    "    print_per = len(train_dialogs)\n",
    "    highest_micro_f1 = 0.\n",
    "\n",
    "    # model\n",
    "    model = EmotionX_Model(hparams)\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), hparams.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_i)\n",
    "    writer = SummaryWriter(log_dir=hparams.log_dir)\n",
    "\n",
    "    # train\n",
    "    for i_epoch in range(hparams.n_epoch):\n",
    "        train_dialogs, train_labels = shuffle_trainset(train_dialogs, train_labels)\n",
    "        scheduler.step()\n",
    "\n",
    "        for i_step in tqdm(range(max_i)):\n",
    "            batch_dialogs = get_batch(train_dialogs, hparams.batch_size, i_step)\n",
    "            batch_labels = get_batch(train_labels, hparams.batch_size, i_step)\n",
    "            optimizer.zero_grad()\n",
    "            pred_labels = model(batch_dialogs)\n",
    "            loss = model.cal_loss(batch_labels, pred_labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), hparams.clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # print\n",
    "            if i_step % print_per == 0:\n",
    "                model.eval()\n",
    "                n_appear = [0] * (hparams.n_class - 1)\n",
    "                n_correct = [0] * (hparams.n_class - 1)\n",
    "                n_positive = [0] * (hparams.n_class - 1)\n",
    "                for i_test in range(len(test_dialogs) // hparams.batch_size):\n",
    "                    batch_dialogs = get_batch(test_dialogs, hparams.batch_size, i_test)\n",
    "                    batch_labels = get_batch(test_labels, hparams.batch_size, i_test)\n",
    "                    pred_labels = model(batch_dialogs)\n",
    "                    counts = model.count_for_eval(batch_labels, pred_labels)\n",
    "                    n_appear = [x + y for x, y in zip(n_appear, counts[0])]\n",
    "                    n_correct = [x + y for x, y in zip(n_correct, counts[1])]\n",
    "                    n_positive = [x + y for x, y in zip(n_positive, counts[2])]\n",
    "                uwa, wa = model.get_uwa_and_wa(n_appear, n_correct)\n",
    "                precision, recall, f1, micro_f1, macro_f1 = model.get_f1_scores(\n",
    "                    n_appear, n_correct, n_positive)\n",
    "\n",
    "                print('i_epoch: ', i_epoch)\n",
    "                print('i_total_step: ', total_step)\n",
    "                print('n_true:\\t\\t\\t', n_appear)\n",
    "                print('n_positive:\\t\\t', n_positive)\n",
    "                print('n_true_positive:\\t', n_correct)\n",
    "                print('precision:\\t[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (\n",
    "                    precision[0], precision[1], precision[2], precision[3], \n",
    "                    precision[4], precision[5], precision[6]))\n",
    "                print('recall:\\t\\t[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (\n",
    "                    recall[0], recall[1], recall[2], recall[3], \n",
    "                    recall[4], recall[5], recall[6]))\n",
    "                print('f1:\\t\\t[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (\n",
    "                    f1[0], f1[1], f1[2], f1[3], f1[4], f1[5], f1[6]))\n",
    "                if micro_f1 > highest_micro_f1:\n",
    "                    highest_micro_f1 = micro_f1\n",
    "                    torch.save(model.state_dict(), \"en_model_save/en_model_save_acc_06_26\" + str(highest_micro_f1))\n",
    "                    friend_high_step = total_step\n",
    "                print('Micro F1: %.4f (<=%.4f at %d-th total_step)'\n",
    "                    % (micro_f1, highest_micro_f1, friend_high_step))\n",
    "                print()\n",
    "\n",
    "                # write\n",
    "                writer.add_scalar(hparams.log_micro_f1+'fr', micro_f1, total_step)\n",
    "                writer.add_scalar(hparams.log_wce_loss+'fr', loss, total_step)\n",
    "                total_step += 1\n",
    "\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6039, 1587, 436, 217, 674, 1506, 308, 2558]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/key13689/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\r",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  0\n",
      "i_total_step:  0\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [929, 0, 1206, 83, 1, 4, 0]\n",
      "n_true_positive:\t [488, 0, 45, 3, 0, 0, 0]\n",
      "precision:\t[0.5253, 0.0000, 0.0373, 0.0361, 0.0000, 0.0000, 0.0000]\n",
      "recall:\t\t[0.3792, 0.0000, 0.5294, 0.0938, 0.0000, 0.0000, 0.0000]\n",
      "f1:\t\t[0.4404, 0.0000, 0.0697, 0.0522, 0.0000, 0.0000, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:39:38,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.2411 (<=0.2411 at 0-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:27<00:00,  4.44it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  1\n",
      "i_total_step:  1\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1285, 367, 0, 0, 0, 571, 0]\n",
      "n_true_positive:\t [1075, 210, 0, 0, 0, 234, 0]\n",
      "precision:\t[0.8366, 0.5722, 0.0000, 0.0000, 0.0000, 0.4098, 0.0000]\n",
      "recall:\t\t[0.8353, 0.6908, 0.0000, 0.0000, 0.0000, 0.8182, 0.0000]\n",
      "f1:\t\t[0.8359, 0.6259, 0.0000, 0.0000, 0.0000, 0.5461, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:39:22,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.6833 (<=0.6833 at 1-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:28<00:00,  4.42it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  2\n",
      "i_total_step:  2\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1101, 435, 168, 0, 199, 320, 0]\n",
      "n_true_positive:\t [1009, 271, 56, 0, 101, 214, 0]\n",
      "precision:\t[0.9164, 0.6230, 0.3333, 0.0000, 0.5075, 0.6687, 0.0000]\n",
      "recall:\t\t[0.7840, 0.8914, 0.6588, 0.0000, 0.6273, 0.7483, 0.0000]\n",
      "f1:\t\t[0.8451, 0.7334, 0.4427, 0.0000, 0.5611, 0.7063, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:39:16,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.7427 (<=0.7427 at 2-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:28<00:00,  4.41it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  3\n",
      "i_total_step:  3\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1075, 412, 231, 0, 202, 303, 0]\n",
      "n_true_positive:\t [1011, 278, 74, 0, 120, 226, 0]\n",
      "precision:\t[0.9405, 0.6748, 0.3203, 0.0000, 0.5941, 0.7459, 0.0000]\n",
      "recall:\t\t[0.7855, 0.9145, 0.8706, 0.0000, 0.7453, 0.7902, 0.0000]\n",
      "f1:\t\t[0.8561, 0.7765, 0.4684, 0.0000, 0.6612, 0.7674, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:38:30,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.7688 (<=0.7688 at 3-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:28<00:00,  4.40it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  4\n",
      "i_total_step:  4\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1087, 394, 234, 0, 154, 354, 0]\n",
      "n_true_positive:\t [1030, 281, 75, 0, 114, 248, 0]\n",
      "precision:\t[0.9476, 0.7132, 0.3205, 0.0000, 0.7403, 0.7006, 0.0000]\n",
      "recall:\t\t[0.8003, 0.9243, 0.8824, 0.0000, 0.7081, 0.8671, 0.0000]\n",
      "f1:\t\t[0.8677, 0.8052, 0.4702, 0.0000, 0.7238, 0.7750, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/920 [00:09<2:20:28,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.7863 (<=0.7863 at 4-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:28<00:00,  4.41it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  5\n",
      "i_total_step:  5\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1106, 345, 132, 0, 325, 315, 0]\n",
      "n_true_positive:\t [1057, 273, 74, 0, 147, 236, 0]\n",
      "precision:\t[0.9557, 0.7913, 0.5606, 0.0000, 0.4523, 0.7492, 0.0000]\n",
      "recall:\t\t[0.8213, 0.8980, 0.8706, 0.0000, 0.9130, 0.8252, 0.0000]\n",
      "f1:\t\t[0.8834, 0.8413, 0.6820, 0.0000, 0.6049, 0.7854, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:38:21,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.8039 (<=0.8039 at 5-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:28<00:00,  4.40it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  6\n",
      "i_total_step:  6\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1156, 392, 124, 0, 214, 337, 0]\n",
      "n_true_positive:\t [1096, 284, 76, 0, 144, 251, 0]\n",
      "precision:\t[0.9481, 0.7245, 0.6129, 0.0000, 0.6729, 0.7448, 0.0000]\n",
      "recall:\t\t[0.8516, 0.9342, 0.8941, 0.0000, 0.8944, 0.8776, 0.0000]\n",
      "f1:\t\t[0.8973, 0.8161, 0.7273, 0.0000, 0.7680, 0.8058, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:38:58,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.8327 (<=0.8327 at 6-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:27<00:00,  4.42it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  7\n",
      "i_total_step:  7\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1194, 369, 131, 0, 208, 321, 0]\n",
      "n_true_positive:\t [1134, 283, 77, 0, 142, 253, 0]\n",
      "precision:\t[0.9497, 0.7669, 0.5878, 0.0000, 0.6827, 0.7882, 0.0000]\n",
      "recall:\t\t[0.8811, 0.9309, 0.9059, 0.0000, 0.8820, 0.8846, 0.0000]\n",
      "f1:\t\t[0.9141, 0.8410, 0.7130, 0.0000, 0.7696, 0.8336, 0.0000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:38:11,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.8498 (<=0.8498 at 7-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:27<00:00,  4.43it/s]\n",
      "  0%|          | 2/920 [00:08<1:33:40,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  8\n",
      "i_total_step:  8\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1168, 377, 115, 0, 217, 345, 1]\n",
      "n_true_positive:\t [1114, 286, 76, 0, 147, 260, 1]\n",
      "precision:\t[0.9538, 0.7586, 0.6609, 0.0000, 0.6774, 0.7536, 1.0000]\n",
      "recall:\t\t[0.8656, 0.9408, 0.8941, 0.0000, 0.9130, 0.9091, 0.0147]\n",
      "f1:\t\t[0.9075, 0.8399, 0.7600, 0.0000, 0.7778, 0.8241, 0.0290]\n",
      "Micro F1: 0.8475 (<=0.8498 at 7-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:27<00:00,  4.44it/s]\n",
      "  0%|          | 0/920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_epoch:  9\n",
      "i_total_step:  9\n",
      "n_true:\t\t\t [1287, 304, 85, 32, 161, 286, 68]\n",
      "n_positive:\t\t [1168, 348, 108, 0, 232, 332, 35]\n",
      "n_true_positive:\t [1124, 286, 78, 0, 149, 258, 11]\n",
      "precision:\t[0.9623, 0.8218, 0.7222, 0.0000, 0.6422, 0.7771, 0.3143]\n",
      "recall:\t\t[0.8733, 0.9408, 0.9176, 0.0000, 0.9255, 0.9021, 0.1618]\n",
      "f1:\t\t[0.9157, 0.8773, 0.8083, 0.0000, 0.7583, 0.8350, 0.2136]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/920 [00:09<1:38:08,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.8574 (<=0.8574 at 9-th total_step)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [03:27<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path):\n",
    "    if not torch.cuda.is_available():\n",
    "        raise NotImplementedError()\n",
    "    hparams = type('', (object,), EMOTIONX_MODEL_HPARAMS)() # dict to class\n",
    "\n",
    "    # data\n",
    "    fr_train_dialogs, fr_train_labels = load_data(hparams, hparams.fr_train_path)\n",
    "    train_dialogs = fr_train_dialogs \n",
    "    train_labels = fr_train_labels\n",
    "    test_dialogs, test_labels = load_data(hparams, hparams.fr_test_path)\n",
    "    assert len(test_dialogs) == len(test_labels)\n",
    "\n",
    "    # hyper-parameter\n",
    "    hparams.n_appear = [sum(train_labels, []).count(i) for i in range(8)]\n",
    "    max_i = len(train_dialogs)\n",
    "    total_step = 0\n",
    "    print_per = len(train_dialogs)\n",
    "    highest_micro_f1 = 0.\n",
    "    hparams.dropout = 0\n",
    "\n",
    "    # model\n",
    "    model = EmotionX_Model(hparams)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), hparams.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_i)\n",
    "    writer = SummaryWriter(log_dir=hparams.log_dir)\n",
    "\n",
    "    # test\n",
    "    n_appear = [0] * (hparams.n_class - 1)\n",
    "    n_correct = [0] * (hparams.n_class - 1)\n",
    "    n_positive = [0] * (hparams.n_class - 1)\n",
    "    for i_test in range(len(test_dialogs) // hparams.batch_size):\n",
    "        batch_dialogs = get_batch(test_dialogs, hparams.batch_size, i_test)\n",
    "        batch_labels = get_batch(test_labels, hparams.batch_size, i_test)\n",
    "        pred_labels = model(batch_dialogs)\n",
    "        counts = model.count_for_eval(batch_labels, pred_labels)\n",
    "        n_appear = [x + y for x, y in zip(n_appear, counts[0])]\n",
    "        n_correct = [x + y for x, y in zip(n_correct, counts[1])]\n",
    "        n_positive = [x + y for x, y in zip(n_positive, counts[2])]\n",
    "        uwa, wa = model.get_uwa_and_wa(n_appear, n_correct)\n",
    "        precision, recall, f1, micro_f1, macro_f1 = model.get_f1_scores(\n",
    "            n_appear, n_correct, n_positive)\n",
    "\n",
    "    print('n_true:\\t\\t\\t', n_appear)\n",
    "    print('n_positive:\\t\\t', n_positive)\n",
    "    print('n_true_positive:\\t', n_correct)\n",
    "    print('precision:\\t[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (\n",
    "        precision[0], precision[1], precision[2], precision[3], \n",
    "        precision[4], precision[5], precision[6]))\n",
    "    print('recall:\\t\\t[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (\n",
    "        recall[0], recall[1], recall[2], recall[3], \n",
    "        recall[4], recall[5], recall[6]))\n",
    "    print('f1:\\t\\t[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (\n",
    "        f1[0], f1[1], f1[2], f1[3], f1[4], f1[5], f1[6]))\n",
    "    print('Micro F1: %.4f ' % (micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"en_model_save/en_model_save_acc_06_260.857399910031489\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(test_path, pretrained_model):\n",
    "    result_file = './Friends/en_sample_0626.csv'\n",
    "    fw = open(result_file, 'w')\n",
    "    wr = csv.writer(fw)\n",
    "    wr.writerow([\"Id\",\"Predicted\"])\n",
    "    \n",
    "    ids = []\n",
    "    sentences = []\n",
    "\n",
    "    with open(test_path, 'r', encoding='CP949') as fr:\n",
    "        rdr = csv.reader(fr)\n",
    "        count_f = 0\n",
    "        for line in rdr:\n",
    "            if count_f == 0:\n",
    "                count_f = 1\n",
    "            else:\n",
    "                ids.append(line[0])\n",
    "                sentences.append(line[1])\n",
    "\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise NotImplementedError()\n",
    "    hparams = type('', (object,), EMOTIONX_MODEL_HPARAMS)() # dict to class\n",
    "    hparams.n_appear = [1, 1, 1, 1, 1, 1, 1, 1] # not to be used\n",
    "    hparams.dropout = 0\n",
    "\n",
    "    print('preprocessing...')\n",
    "    tokenizer = BertTokenizer.from_pretrained(hparams.bert_type)\n",
    "    inputs = []\n",
    "    for dialog in sentences:\n",
    "        tokenized_dialog = []\n",
    "        tokenized_utter = tokenizer.tokenize(dialog.lower())\n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized_utter)\n",
    "        if len(tokenized_dialog + tokenized_utter) + 1 > hparams.max_input_len:\n",
    "            print('[CAUTION] over max_input_len: ', utter['utterance'])\n",
    "            continue\n",
    "        tokenized_dialog += tokenized_ids + [hparams.sep_id]\n",
    "        inputs.append(tokenized_dialog)\n",
    "  \n",
    "    print('prediction...')\n",
    "    model = EmotionX_Model(hparams)\n",
    "    model.load_state_dict(torch.load(pretrained_model))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    for i_test in range(len(inputs) // hparams.batch_size):\n",
    "        batch = get_batch(inputs, hparams.batch_size, i_test)      \n",
    "        logits = model(batch)[:, :-1] # trim the OOD column\n",
    "        _, pred = torch.max(logits, dim=1)\n",
    "        pred_list += pred.tolist()\n",
    "    assert sum(inputs, []).count(102) == len(pred_list) # n_utter == n_pred\n",
    "\n",
    "    print('labeling...')\n",
    "    index_to_emotion = {0: 'neutral', 1: 'joy', 2: 'sadness',3: 'fear', \n",
    "                      4: 'anger', 5: 'surprise', 6: 'disgust', 7: 'non-neutral'}\n",
    "    \n",
    "    for i in range(len(pred_list)):\n",
    "        pred_str = index_to_emotion[pred_list[i]]\n",
    "        wr.writerow([ids[i], pred_str])\n",
    "\n",
    "            \n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "prediction...\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labeling...\n"
     ]
    }
   ],
   "source": [
    "make_sample('Friends/en_sample.csv', \"en_model_save/en_model_save_acc_06_200.8547008547008547\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
